# ğŸ§  SOCIAL MEDIA CONTEXT MODERIZATION

A **BERT-powered AI system** that automatically detects and filters **toxic, offensive, or inappropriate text** from social media posts and comments.  
This project applies **Natural Language Processing (NLP)** and **Machine Learning (ML)** to ensure respectful online communication.

---

## ğŸ¯ Objective

The goal of this project is to **analyze user-generated text** and predict whether it contains **toxic or harmful language** (like hate speech, insults, or threats).  
It helps moderators and platforms automatically flag or remove inappropriate content.

---

## ğŸš€ Features

- âœ… Detects multiple categories of toxicity â€” *toxic, severe toxic, obscene, threat, insult, identity hate, etc.*
- âš™ï¸ Uses **BERT (Bidirectional Encoder Representations from Transformers)** for deep text understanding
- ğŸ§¹ Includes **advanced text preprocessing** â€” tokenization, lemmatization, stopword & emoji handling
- ğŸŒ Has a **FastAPI backend** and a **simple HTML frontend** for user interaction
- ğŸ“¦ Easy to deploy locally or on cloud platforms like **AWS**, **GCP**, or **Hugging Face Spaces**
- ğŸ§  Includes a **Jupyter Notebook** for training and fine-tuning models

  ---

## ğŸ§© Project Structure

socialmediatextmoderization/
â”‚
â”œâ”€â”€ Backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ toxic_bert_model/
â”‚ â”‚ â”œâ”€â”€ config.json
â”‚ â”‚ â”œâ”€â”€ tokenizer.json
â”‚ â”‚ â””â”€â”€ pytorch_model.bin
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â””â”€â”€ preprocessing.py
â”‚ â””â”€â”€ models/
â”‚ â””â”€â”€ text_model.py
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ index.html
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ text/
â”‚ â”œâ”€â”€ combined_dataset.csv
â”‚ â””â”€â”€ other_dataset_files
â”‚
â”œâ”€â”€ text_combine_csv.py
â””â”€â”€ text.ipynb


---

## ğŸ§  `Backend/main.py`

This is the **main FastAPI application** that connects the model with the user interface.

### ğŸ“ What it does:
- Loads the **BERT model** and **tokenizer** from `toxic_bert_model/`
- Accepts input text from the frontend (via `POST /predict`)
- Cleans and preprocesses the text using `utils/preprocessing.py`
- Passes the cleaned text to the model (`models/text_model.py`)
- Returns the prediction result as JSON

### ğŸ’» Example Code Snippet:

@app.post("/predict")
def predict_text(data: TextInput):
    cleaned_text = preprocess_text(data.text)
    prediction = model.predict(cleaned_text)
    return {"final_label": prediction}


âš™ï¸ How it works (Step-by-Step):

User enters text in the web form.
The text is sent to the backend via the /predict endpoint.
The backend cleans the text â†’ sends it to the BERT model.
Model predicts toxicity category (e.g., â€œtoxicâ€, â€œcleanâ€).
The result is returned to the frontend for display.

ğŸ§° Backend/utils/preprocessing.py

This module handles text cleaning and preparation before the data is sent to the model.

ğŸ§¹ Operations Performed:

Converts text to lowercase
Removes URLs, HTML tags, emojis, and punctuation
Tokenizes sentences into words
Removes stopwords
Lemmatizes words to their base form

ğŸ’» Example Code:
def preprocess_text(text):
    text = re.sub(r"http\S+", "", text)     # Remove URLs
    text = text.lower()
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stopwords]
    return " ".join(tokens)

ğŸ¯ Purpose:

Makes input text clean and consistent, improving the accuracy of the BERT model by reducing noise and irrelevant patterns.

ğŸ§© Backend/models/text_model.py

Defines and loads the BERT model used for prediction.

âš™ï¸ What Happens Here:

Loads the pretrained BERT model and tokenizer
Converts text into token IDs understandable by BERT
Passes data through the model to get prediction probabilities
Maps model output to human-readable labels (like â€œtoxicâ€)

ğŸ’» Example Logic:
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
prediction = torch.sigmoid(outputs.logits)

ğŸ”„ Working:

Text â†’ Tokenized into numerical IDs

BERT â†’ Generates contextual embeddings

Classification layer â†’ Predicts toxicity probabilities

ğŸ’» frontend/index.html

A simple web interface that allows users to enter text and see the toxicity result.

ğŸŒ Features:

Input box for text
â€œAnalyzeâ€ button to submit
Displays modelâ€™s toxicity prediction dynamically

ğŸ’» Example Snippet:
<form id="predict-form">
  <textarea id="user-input" placeholder="Enter text here..."></textarea>
  <button type="submit">Analyze</button>
</form>
<div id="result"></div>

âš™ï¸ Working:

When the user clicks â€œAnalyze,â€ the frontend sends the text to /predict API.
The backend processes it and returns the result (e.g., Toxic / Not Toxic).

ğŸ“Š data/text/combined_dataset.csv

Contains the training dataset used for fine-tuning the model.
Each row includes text and corresponding labels for toxicity classes.

Example columns:

text, toxic, severe_toxic, obscene, threat, insult, identity_hate
Used by text.ipynb during training.

ğŸ§® text_combine_csv.py

A simple Python script that merges multiple CSV datasets into one large dataset for model training.

ğŸ’» Example Logic:
import pandas as pd

files = ["toxic.csv", "comments.csv"]
combined = pd.concat([pd.read_csv(f) for f in files])
combined.to_csv("data/text/combined_dataset.csv", index=False)

ğŸ¯ Purpose:

Helps combine datasets from different sources efficiently into one file.

ğŸ““ text.ipynb

Jupyter Notebook used to train and fine-tune the BERT model.

ğŸ§  Steps Performed:

Load dataset (combined_dataset.csv)
Preprocess text data
Tokenize text using BERT tokenizer
Train classification layers
Save trained model files into Backend/toxic_bert_model/

ğŸ’» Example Snippet:
from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/konduru-rohith09/socialmediatextmoderization.git
cd socialmediatextmoderization

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
venv\Scripts\activate   # On Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


If requirements.txt is missing:

pip install fastapi uvicorn torch transformers pandas numpy scikit-learn spacy nltk emoji
python -m spacy download en_core_web_sm

â–¶ï¸ Running the Application
ğŸ–¥ï¸ Backend (FastAPI)
cd Backend
uvicorn main:app --reload


â¡ Server runs at: http://127.0.0.1:8000

ğŸŒ Frontend

Open your browser and visit the above URL to use the text moderation interface.

ğŸ§° Troubleshooting
Issue	Solution
Model not found	Ensure Backend/toxic_bert_model/ contains files like pytorch_model.bin
TemplateNotFound: index.html	Place index.html inside the frontend/ folder
nltk data missing	Run:
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

ğŸŒŸ Future Enhancements

ğŸŒ Multilingual text moderation
ğŸ“Š Real-time toxicity analytics dashboard
ğŸš€ Docker / Streamlit / Hugging Face Spaces deployment
ğŸ’¡ Explainable AI (highlight toxic words in sentences)

ğŸ¤ Contributing

Fork this repository

Create a feature branch:
  git checkout -b feature-name

Commit changes:
  git commit -m "Added new feature"


Push to your branch:
  git push origin feature-name

Open a Pull Request ğŸš€

ğŸ“œ License

Licensed under the MIT License.
See the LICENSE file for more information.

ğŸ‘¨â€ğŸ’» Author

Konduru Rohith
ğŸ“ GitHub: @konduru-rohith09

ğŸ’¬ Acknowledgements

ğŸ¤— Hugging Face Transformers
âš¡ FastAPI Documentation
ğŸ§  spaCy NLP Toolkit
ğŸ“š NLTK Python Library





